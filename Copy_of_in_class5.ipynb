{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of in-class5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EDV5a03DU2c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a648631d-b1bd-4936-f87b-6293aadb2d21"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "import csv\n",
        "\n",
        "url ='https://www.learndatasci.com/free-data-science-books/'\n",
        "pageContent = requests.get(url)\n",
        "page = BeautifulSoup(pageContent.content, 'html.parser')\n",
        "#print(page)\n",
        "\n",
        "with open(\"/content/sample_data/booklist.csv\", \"w\") as book:\n",
        "  booklist = csv.writer(book)\n",
        "  booklist.writerow([\"Name\"])\n",
        "  for url in page.find_all('h2'):\n",
        "    print(url.string)\n",
        "    booklist.writerow([url.string])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Artificial Intelligence A Modern Approach, 1st Edition\n",
            "Learning Deep Architectures for AI\n",
            "The LION Way: Machine Learning plus Intelligent Optimization\n",
            "Big Data Now: 2012 Edition\n",
            "Disruptive Possibilities: How Big Data Changes Everything\n",
            "Real-Time Big Data Analytics: Emerging Architecture\n",
            "Computer Vision\n",
            "Natural Language Processing with Python\n",
            "Programming Computer Vision with Python\n",
            "The Elements of Data Analytic Style\n",
            "A Course in Machine Learning\n",
            "A First Encounter with Machine Learning\n",
            "Algorithms for Reinforcement Learning\n",
            "A Programmer's Guide to Data Mining\n",
            "Bayesian Reasoning and Machine Learning\n",
            "Data Mining Algorithms In R\n",
            "Data Mining and Analysis: Fundamental Concepts and Algorithms\n",
            "Data Mining: Practical Machine Learning Tools and Techniques\n",
            "Data Mining with Rattle and R\n",
            "Deep Learning\n",
            "Gaussian Processes for Machine Learning\n",
            "Information Theory, Inference, and Learning Algorithms\n",
            "Introduction to Machine Learning\n",
            "Introduction to Machine Learning\n",
            "KB – Neural Data Mining with Python Sources\n",
            "Machine Learning\n",
            "Machine Learning, Neural and Statistical Classification\n",
            "Machine Learning – The Complete Guide\n",
            "Mining of Massive Datasets\n",
            "Modeling With Data\n",
            "Neural Networks and Deep Learning\n",
            "Probabilistic Programming & Bayesian Methods for Hackers\n",
            "Real-World Active Learning\n",
            "Reinforcement Learning: An Introduction\n",
            "Social Media Mining An Introduction\n",
            "Theory and Applications for Advanced Text Mining\n",
            "Understanding Machine Learning: From Theory to Algorithms\n",
            "An Introduction to Data Science\n",
            "Data Jujitsu: The Art of Turning Data into Product\n",
            "School of Data Handbook\n",
            "The Art of Data Science\n",
            "D3 Tips and Tricks\n",
            "Interactive Data Visualization for the Web\n",
            "Cloudera Impala\n",
            "Data-Intensive Text Processing with MapReduce\n",
            "Hadoop Illuminated\n",
            "Hadoop Tutorial as a PDF\n",
            "Programming Pig\n",
            "Building Data Science Teams\n",
            "Data Driven: Creating a Data Culture\n",
            "Understanding the Chief Data Officer\n",
            "The Data Analytics Handbook\n",
            "The Data Science Handbook\n",
            "A Byte of Python\n",
            "Advanced R\n",
            "A Little Book of R for Time Series\n",
            "Automate the Boring Stuff with Python: Practical Programming for Total Beginners\n",
            "Dive Into Python 3\n",
            "Ecological Models and Data in R\n",
            "Invent with Python\n",
            "Learning Statistics with R\n",
            "Learning with Python 3\n",
            "Learn Python, Break Python\n",
            "Learn Python the Hard Way\n",
            "Practical Regression and Anova using R\n",
            "Python Cookbook\n",
            "Python for Informatics: Exploring Information\n",
            "Python for You and Me\n",
            "Python Practice Book\n",
            "Python Programming\n",
            "R by Example\n",
            "R Programming\n",
            "R Programming for Data Science\n",
            "Spatial Epidemiology Notes: Applications and Vignettes in R\n",
            "Test-Driven Development with Python\n",
            "The R Inferno\n",
            "The R Manuals\n",
            "Think Python 2nd Edition\n",
            "A First Course in Linear Algebra\n",
            "Elementary Applied Topology\n",
            "Elementary Differential Equations\n",
            "Introduction to Probability\n",
            "Linear Algebra\n",
            "Linear Algebra: An Introduction to Mathematical Discourse\n",
            "Linear Algebra, Theory And Applications\n",
            "Ordinary Differential Equations\n",
            "Probabilistic Models in the Study of Language\n",
            "Probability and Statistics Cookbook\n",
            "Cassandra Tutorial as a PDF\n",
            "CouchDB: The Definitive Guide\n",
            "Extracting Data from NoSQL Databases\n",
            "Graph Databases\n",
            "Learn SQL The Hard Way\n",
            "MongoDB Succinctly\n",
            "NoSQL Databases\n",
            "SQL for Web Nerds\n",
            "SQL Tutorial as a PDF\n",
            "The Little MongoDB Book\n",
            "A First Course in Design and Analysis of Experiments\n",
            "An Introduction to Statistical Learning with Applications in R\n",
            "Artificial Intelligence: Foundations of Computational Agents\n",
            "Intro Stat with Randomization and Simulation\n",
            "OpenIntro Statistics\n",
            "The Elements of Statistical Learning: Data Mining, Inference, and Prediction\n",
            "Think Bayes: Bayesian Statistics Made Simple\n",
            "Think Stats: Exploratory Data Analysis in Python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCPtlDkYEARL",
        "colab_type": "code",
        "outputId": "f54ef971-5fc0-429a-f43b-b830ce4dfea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Span\n",
        "from spacy import displacy\n",
        "\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Linear Algebra: An Introduction to Mathematical Discourse\"\n",
        "text = \"Understanding Machine Learning: From Theory to Algorithms\"  \n",
        "\n",
        "doc =nlp(text)\n",
        "\n",
        "for tok in doc:\n",
        "  print(tok.text, \"-->\",tok.dep_,\"-->\", tok.pos_)\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{'LOWER': 'Map', 'POS': 'PROPN'},{'LOWER': 'multicore', 'POS': 'NOUN'}]\n",
        "\n",
        "matcher.add(\"Matching\", None, pattern)\n",
        "\n",
        "matches = matcher(doc)\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "  \n",
        "    string_id = nlp.vocab.strings[match_id]  \n",
        "    span = doc[start:end]  \n",
        "    print(match_id, string_id, start, end, span.text)\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Understanding --> ROOT --> VERB\n",
            "Machine --> compound --> NOUN\n",
            "Learning --> dobj --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "From --> prep --> ADP\n",
            "Theory --> pobj --> NOUN\n",
            "to --> prep --> ADP\n",
            "Algorithms --> pobj --> PROPN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhth-gh-Gl-V",
        "colab_type": "code",
        "outputId": "ddd2e432-63bf-44b2-ea28-a6d044d0a58d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install lexnlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lexnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/69/754eae7606ed58f3d5e11f86c9ced6a0c99ecaab460e628af62d7e0e2202/lexnlp-1.4.0-py3-none-any.whl (9.7MB)\n",
            "\u001b[K     |████████████████████████████████| 9.7MB 3.1MB/s \n",
            "\u001b[?25hCollecting requests==2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n",
            "\u001b[?25hCollecting reporters-db==1.0.12.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/6c/16c7c3849a25d2c3af5ef6e05d768d8e86e74aa9051df4728325c5f31f46/reporters_db-1.0.12.1-py2.py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.3MB/s \n",
            "\u001b[?25hCollecting pandas==0.23.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/d8/feeb346d41f181e83fba45224ab14a8d8af019b48af742e047f3845d8cff/pandas-0.23.4-cp36-cp36m-manylinux1_x86_64.whl (8.9MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9MB 43.3MB/s \n",
            "\u001b[?25hCollecting pycountry==18.5.26\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/c0/8ce9d2b55347867900edbe4d18f790571130c16f882b4891a0f08627dcdc/pycountry-18.5.26-py2.py3-none-any.whl (10.3MB)\n",
            "\u001b[K     |████████████████████████████████| 10.3MB 37.5MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.21.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 36.6MB/s \n",
            "\u001b[?25hCollecting typing==3.6.2\n",
            "  Downloading https://files.pythonhosted.org/packages/44/88/d09c6a7fe1af4a02f16d2f1766212bec752aadb04e5699a9706a10a1a37d/typing-3.6.2-py3-none-any.whl\n",
            "Collecting dateparser==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/9e/1aa87c0c59f9731820bfd20a8b148d97b315530c2c92d1fb300328c8c42f/dateparser-0.7.0-py2.py3-none-any.whl (357kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 52.0MB/s \n",
            "\u001b[?25hCollecting us==1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/72/83/8731cbf5afcf3434c0b24cfc520c11fd27bfc8a6878114662f4e3dbdab71/us-1.0.0.tar.gz\n",
            "Collecting gensim==3.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/33/df6cb7acdcec5677ed130f4800f67509d24dbec74a03c329fcbf6b0864f0/gensim-3.4.0-cp36-cp36m-manylinux1_x86_64.whl (22.6MB)\n",
            "\u001b[K     |████████████████████████████████| 22.6MB 1.6MB/s \n",
            "\u001b[?25hCollecting num2words==0.5.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/9a/31a9151abd891ab7387d8d74cb0d84c4e77674735dbf85a63dfeb8eed6a6/num2words-0.5.7.tar.gz (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.8MB/s \n",
            "\u001b[?25hCollecting Unidecode==0.4.21\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/a1/9d7f3138ee3d79a1ab865a2cb38200ca778d85121db19fe264c76c981184/Unidecode-0.04.21-py2.py3-none-any.whl (228kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 39.1MB/s \n",
            "\u001b[?25hCollecting scipy==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/5e/caa01ba7be11600b6a9d39265440d7b3be3d69206da887c42bef049521f2/scipy-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (50.0MB)\n",
            "\u001b[K     |████████████████████████████████| 50.0MB 76kB/s \n",
            "\u001b[?25hCollecting regex==2017.9.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/65/91b43adad1dc45d7374521422270490128a2f289e1c3e1036b231b521507/regex-2017.09.23.tar.gz (607kB)\n",
            "\u001b[K     |████████████████████████████████| 614kB 48.6MB/s \n",
            "\u001b[?25hCollecting datefinder-lexpredict==0.6.2\n",
            "  Downloading https://files.pythonhosted.org/packages/27/47/9a38724045b30e2e4d1c5e3e08fd3b0770dedb2e9ca92c1347b9e2182470/datefinder_lexpredict-0.6.2-py2.py3-none-any.whl\n",
            "Collecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from reporters-db==1.0.12.1->lexnlp) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.4->lexnlp) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.4->lexnlp) (1.18.3)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.4->lexnlp) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3->lexnlp) (0.14.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from dateparser==0.7.0->lexnlp) (1.5.1)\n",
            "Collecting jellyfish==0.5.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/48/ddb1458d966f0a84e472d059d87a9d1527df7768a725132fc1d810728386/jellyfish-0.5.6.tar.gz (132kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.4.0->lexnlp) (1.11.1)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.4.0->lexnlp) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.4.0->lexnlp) (1.12.43)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.43 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (1.15.43)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (0.15.2)\n",
            "Building wheels for collected packages: us, num2words, regex, nltk, jellyfish\n",
            "  Building wheel for us (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for us: filename=us-1.0.0-cp36-none-any.whl size=11833 sha256=48af4b9c8e34a296a36e93dee39c171fbce6160fe6942fb66c2385e73604e39f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/98/40/cb8be35d7779a0ae4372c84e7a585c947bfc41540fd8999e53\n",
            "  Building wheel for num2words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for num2words: filename=num2words-0.5.7-cp36-none-any.whl size=84777 sha256=8d37de7ba778df615453bc75ad96fba5465a368b600b6ce2ae6bc7d63ed20fa4\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/f6/a1/78cb080e8529bfa0c6ee2461563a4ff96efc0b6fe89cb31457\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.9.23-cp36-cp36m-linux_x86_64.whl size=538828 sha256=bf2a0bd2fa896219517a07fbe7fd32cd32ca3f8a25af8f1f3411a1546d3b139d\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/19/41/e7d239b4a53386fe9de49f9e4328799569bbeac8b8b3748876\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449905 sha256=9193b5399884eed9d2f19d70411caff6ac6504d0a96a5cc0ed06f81f12ba1e76\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.5.6-cp36-cp36m-linux_x86_64.whl size=74547 sha256=cec66ff08fc80cc681b3b915418201cfb68c923db1c037551d60592061dabc92\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/29/06/8d686d24f742cb89e7bde7f26f18cb9e89b3c8bcd6999cb12a\n",
            "Successfully built us num2words regex nltk jellyfish\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement pandas>=0.25, but you'll have pandas 0.23.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc3 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.10.0 has requirement scipy>=1.0.1, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.23.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.23.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.0.0; python_version >= \"3.0\", but you'll have pandas 0.23.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.21.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement scipy>=1.1.0, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: requests, reporters-db, pandas, pycountry, scipy, scikit-learn, typing, regex, dateparser, jellyfish, us, gensim, num2words, Unidecode, datefinder-lexpredict, nltk, lexnlp\n",
            "  Found existing installation: requests 2.21.0\n",
            "    Uninstalling requests-2.21.0:\n",
            "      Successfully uninstalled requests-2.21.0\n",
            "  Found existing installation: pandas 1.0.3\n",
            "    Uninstalling pandas-1.0.3:\n",
            "      Successfully uninstalled pandas-1.0.3\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: typing 3.6.6\n",
            "    Uninstalling typing-3.6.6:\n",
            "      Successfully uninstalled typing-3.6.6\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed Unidecode-0.4.21 datefinder-lexpredict-0.6.2 dateparser-0.7.0 gensim-3.4.0 jellyfish-0.5.6 lexnlp-1.4.0 nltk-3.4.5 num2words-0.5.7 pandas-0.23.4 pycountry-18.5.26 regex-2017.9.23 reporters-db-1.0.12.1 requests-2.22.0 scikit-learn-0.21.3 scipy-1.0.0 typing-3.6.2 us-1.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk",
                  "pandas",
                  "regex",
                  "requests",
                  "scipy",
                  "sklearn",
                  "typing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTCq_muAEAAW",
        "colab_type": "code",
        "outputId": "0cbb71e3-51ad-413f-adf5-cd9cdb50d6c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#!pip install lexnlp\n",
        "import nltk\n",
        "import spacy\n",
        "import lexnlp.extract.en.acts\n",
        "\n",
        "to_do_list=[]\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        " for line in x:\n",
        "       cool = line.split(\" \\n \")\n",
        "       acts = lexnlp.extract.en.acts.get_act_list(line)\n",
        "       if len(acts)>0:\n",
        "         acts_list.append(acts)\n",
        "         print(acts)\n",
        "if len(to_do_list) == 0:\n",
        "  print(None)\n",
        "else:\n",
        "  print(to_do_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea7SJO3UHWhg",
        "colab_type": "code",
        "outputId": "f2f5123b-7022-457d-ef80-691eb1bf6c6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "import lexnlp.extract.en.amounts\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        "  for line in x:\n",
        "       cool = line.split(\" \\n \")\n",
        "       amounts =list(lexnlp.extract.en.amounts.get_amounts(line))\n",
        "       if len(amounts)>0:\n",
        "           print(amounts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[5.0, 740.0]\n",
            "[1843.0]\n",
            "Warning: parsing empty text\n",
            "Warning: parsing empty text\n",
            "[2.0]\n",
            "Warning: parsing empty text\n",
            "Warning: parsing empty text\n",
            "[1.0]\n",
            "[4.0]\n",
            "Warning: parsing empty text\n",
            "Warning: parsing empty text\n",
            "[2.0]\n",
            "[1821.0]\n",
            "[5.0]\n",
            "[1.0, 1840.0, 3777, 80.0, 100.0, 30, 1839.0, 741.0, 22, 1840.0, 14000, 120, 1, 1840.0, 3, 4]\n",
            "[1]\n",
            "[1.0, 1840.0, 2.0, 1.0, 361.0, 1.0, 307.0, 6.0, 604.0, 1.0, 2.0, 418.0, 422.0, 7.0, 34.0, 41.0, 167.0, 742.0, 3.0, 112.0, 207.0, 3.0, 338.0, 424.0, 5.0, 26.0, 13.0, 235.0, 8.0, 693.0, 4.0]\n",
            "[1821.0, 167.0]\n",
            "Warning: parsing empty text\n",
            "Warning: parsing empty text\n",
            "[2.0, 2.0, 216.0, 3.0, 66.0, 4.0, 130.0]\n",
            "[29.0, 2.0, 241.0, 2.0, 332.0, 2.0, 422.0, 9.0, 112.0, 743.0, 9.0, 39.0, 14000]\n",
            "[1840.0]\n",
            "Warning: parsing empty text\n",
            "Warning: parsing empty text\n",
            "[744.0, 5.0, 182.0]\n",
            "[3.0, 368.0, 1.0, 397.0, 6.0, 604.0, 1, 1821.0, 167.0, 745.0]\n",
            "[4.0]\n",
            "[746.0]\n",
            "Warning: parsing empty text\n",
            "Warning: parsing empty text\n",
            "[4.0, 210.0, 46.0]\n",
            "[747.0]\n",
            "[5.0]\n",
            "[5.0, 740.0, 1843.0, 284.0]\n",
            "[2019.0]\n",
            "Warning: parsing empty text\n",
            "Warning: parsing empty text\n",
            "[9.0]\n",
            "[1.0]\n",
            "[55.0, 266.0, 271.0]\n",
            "[1876.0]\n",
            "[2.0]\n",
            "[47.0, 362.0, 376.0]\n",
            "[1872.0]\n",
            "[3.0]\n",
            "[45.0, 329.0, 334.0]\n",
            "[1871.0]\n",
            "[4.0]\n",
            "[31.0, 526.0, 527.0]\n",
            "[1858.0]\n",
            "[5.0]\n",
            "[21.0, 333.0, 335.0]\n",
            "[1852.0]\n",
            "[6.0]\n",
            "[8.0, 145.0, 147.0]\n",
            "[1857.0]\n",
            "[7.0]\n",
            "[65.0, 256.0, 258.0]\n",
            "[3]\n",
            "[1880.0]\n",
            "[8.0]\n",
            "[4.0, 913.0, 914.0]\n",
            "[1887.0]\n",
            "[9.0]\n",
            "[103.0, 464.0]\n",
            "[1936.0]\n",
            "Warning: parsing empty text\n",
            "Warning: parsing empty text\n",
            "[3.0]\n",
            "[1.0]\n",
            "[9.0, 39.0, 1828.0]\n",
            "[2.0]\n",
            "[2.0]\n",
            "[5.0, 182.0, 1837.0]\n",
            "Warning: parsing empty text\n",
            "Warning: parsing empty text\n",
            "[2.0]\n",
            "[3.0]\n",
            "[9.0, 108.0, 1812.0]\n",
            "[6, 1]\n",
            "[2.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YpldUFzHqJj",
        "colab_type": "code",
        "outputId": "8684147d-8f54-46c0-f3b3-8e13e7c3c66a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "import lexnlp.extract.en.citations\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        " for line in x:\n",
        "       cool = line.split(\" \\n \")\n",
        "       references = list(lexnlp.extract.en.citations.get_citations(line))\n",
        "       if len(references)>0:\n",
        "           print(references)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(5, 'Ala.', 'Alabama Reports', 740, None, None, None)]\n",
            "[(5, 'Ala.', 'Alabama Reports', 740, '1843', None, None)]\n",
            "[(55, 'Ala.', 'Alabama Reports', 266, '271', None, None)]\n",
            "[(47, 'Ala.', 'Alabama Reports', 362, '376', None, None)]\n",
            "[(45, 'Ala.', 'Alabama Reports', 329, '334', None, None)]\n",
            "[(31, 'Ala.', 'Alabama Reports', 526, '527', None, None)]\n",
            "[(21, 'Ala.', 'Alabama Reports', 333, '335', None, None)]\n",
            "[(8, 'Cal.', 'California Reports', 145, '147', None, None)]\n",
            "[(65, 'Ala.', 'Alabama Reports', 256, '258', None, None)]\n",
            "[(4, 'S.W.', 'South Western Reporter', 913, '914', None, None)]\n",
            "[(103, 'A.L.R.', 'American Law Reports', 464, None, None, None)]\n",
            "[(9, 'Cow.', \"Cowen's Reports\", 39, None, None, None)]\n",
            "[(5, 'Port.', 'Alabama Reports, Porter', 182, None, None, None)]\n",
            "[(9, 'Johns.', \"Johnson's Reports\", 108, None, None, None)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DDlhBMZIz-F",
        "colab_type": "code",
        "outputId": "29ed2082-2e81-4948-c7a5-32f3900e9bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import lexnlp.extract.en.entities.nltk_re\n",
        "\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        " for line in x:\n",
        "       cool = line.split(\" \\n \")\n",
        "       companies = list(lexnlp.extract.en.entities.nltk_re.get_companies(line))\n",
        "       if len(companies)>0:\n",
        "          print(companies)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Lehman, Durr Co, (4, 22)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AovUVlPUJFAc",
        "colab_type": "code",
        "outputId": "c0c2fffd-8d65-4035-c139-1c5c6f093885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "import lexnlp.extract.en.conditions\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        " for line in x:\n",
        "       cool = line.split(\" \\n \")\n",
        "       conditions = list(lexnlp.extract.en.conditions.get_conditions(line))\n",
        "       if len(conditions)>0:\n",
        "          print(conditions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('until', 'Under St.1821, prohibiting a levy on a crop', ''), ('until', 'on a growing crop, nor does such lien attach', '')]\n",
            "[('if', 'It was proved by the claimants, by the production of a written contract, that Harrison, on the twenty-second of May, 1840, in consideration that the claimants were involved, as indorsers for Burton & Harrison of Sumter county, and were then exposed to an execution, amounting to upwards of fourteen thousand dollars, bargained and sold to the claimants all his growing crop of cotton &c., consisting of one hundred and twenty acres, &c. Allen Harrison promised and obliged himself to give up his crop to the use of the claimants at any time to save them from suffering as his indorsers;', ''), ('when', 'The claimants came from Tennessee, (where they resided) about the first of September, 1840, bringing with them three or four white laborers, and took possession of the crop and slaves, and with the latter, and white laborers, gathered the cotton, prepared it for market, and', '')]\n",
            "[('if', 'The court charged the jury, that the plaintiff had no lien by virtue of his judgment, and execution on the growing crop; that Harrison had a right to convey it, without being in any manner restrained by them; that the writing adduced, was a sale of the crop, but', ''), ('when', 'it was not, and the lien of the fieri facias would have attached upon it,', ''), ('if', 'gathered, yet', ''), ('not subject to', 'the claimants obtained possession on the first of September, and controlled the gathering of the crop, then no lien attached, and it was', '')]\n",
            "[('until', 'Rep, 693;] and', '')]\n",
            "[('until', '167,] which declares it to be lawful to levy an execution on a planted crop,', '')]\n",
            "[('if', 'It is admitted that the contract between the defendant in execution, and the claimants, was in good faith,', '')]\n",
            "[('when', 'The defendant in execution might at any time have divested the interest which the contract vested in the claimants, by discharging their liability as his indorsers, or a judgment creditor might have satisfied the lien, and', '')]\n",
            "[('unless', 'We will then consider the writing under which the claimants assert a right, as a mortgage with a power to take possession any time during the year,', '')]\n",
            "[('if', 'Conceding the truth of the facts stated in the bill of exceptions, and we think it will not follow, that the possession of the claimants is a nullity, and that the case must be considered as', ''), ('if', 'The contract contains an express undertaking to give up the crop at any time the claimants might require it for their indemnity, and', ''), ('if', 'they took possession of it in the absence of the grantor, (though without his consent,)', ''), ('if', 'he subsequently acquiesced in it, the inference would be,', '')]\n",
            "[('subject to', 'Mr. Dane, in remarking upon this point, says, “The American editor of Bacon’s Abridgment, says, ‘Wheat growing in the ground is a chattel, and', ''), ('until', 'The first section of the act of 1821, “To prevent sheriffs and other officers from levying executions in certain cases, enacts, that “It shall not be lawful for any sheriff or other officer, to levy a writ of fieri facias or other execution on the planted crop of a debtor, or person against whom an execution may issue,', ''), ('until', 'Now here is an express inhibition to levy an execution on a crop while it remains on, or in the ground, and', ''), ('until', 'If so, the act cited, will only have the effect of keeping the right to levy it in abeyance', ''), ('if', 'The lien and the right to levy are intimately connected, and', ''), ('until', 'That it was competent for the legislature to have made it unlawful to levy an execution on particular property,', ''), ('until', 'If the object was merely to suspend the sale,', ''), ('as soon as', 'The idea that the lien attached upon the planted crop', ''), ('until', 'the execution was delivered to the sheriff, though the right to levy it was postponed', ''), ('if', 'They do not refer to the lien,', ''), ('until', 'they did they would postpone it', ''), ('until', 'the crop was gathered; but it is the levy they relate to and postpone', '')]\n",
            "[('until', '**4 The right to levy an execution on a planted crop, then, being expressly taken away by the statute, the lien which is connected with and consequent upon that right, never attaches', '')]\n",
            "[('if', 'The circuit judge may have mistaken the law in supposing that the contract was a sale, but', ''), ('when', 'There is no assumption of any material fact in the charge; but the possession of the claimant, the time', ''), ('if', 'acquired, the gathering of the crop, &c., are all referred to the determination of the jury; who are instructed,', '')]\n",
            "[('until', '**4 The statute which presents the question before the court is, that “it shall not be lawful for any sheriff or other officer to levy a writ of fieei facias or other execution, on the planted crop of a debtor, or person against whom an execution may issue,', '')]\n",
            "[('subject to', 'The policy of the State, as indicated by these statutes, is undeniably that all the property of a debtor, real and personal, to which he has a legal title, shall be', ''), ('until', 'The mischief which the statute designed to remedy was, the sacrifice which would be necessarily made by the sale of an immature crop: the statute enables the debtor to retain it', '')]\n",
            "[('if', '**5', ''), ('until', 'The sheriff is forbidden to levy on a “planted crop”', ''), ('if', 'Now,', ''), ('until', 'This, I feel a thorough conviction, was not the intention of the legislature; but that it was to secure him from loss, by prohibiting a levy and sale of the crop,', ''), ('when', 'it was gathered,', '')]\n",
            "[('subject to', 'Growing crops as', '')]\n",
            "[('subject to', 'Generally, at common law, growing crops raised by annual planting, while still attached to the soil, are regarded as personal chattels,', '')]\n",
            "[('where', 'And', '')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KToNHBLsJhOP",
        "colab_type": "code",
        "outputId": "29d62dc6-cef0-4887-a580-78f819c5e1a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import lexnlp.extract.en.constraints\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        " for line in x:\n",
        "       cool = line.split(\" \\n \")\n",
        "       contraints = list(lexnlp.extract.en.constraints.get_constraints(line))\n",
        "       if len(contraints)>0:\n",
        "          print(contraints)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('after', 'on a growing crop, nor does such lien attach until', '')]\n",
            "[('after', '', ' and that alias and pluries fieri facias’, issued regularly up to the time levy was made; that the cotton levied on was growed on the plantation of harrison, and cultivated by the hands in his service.'), ('first of', 'the claimants came from tennessee, (where they resided) about the', '')]\n",
            "[('first of', 'the court charged the jury, that the plaintiff had no lien by virtue of his judgment, and execution on the growing crop; that harrison had a right to convey it, without being in any manner restrained by them; that the writing adduced, was a sale of the crop, but if it was not, and the lien of the fieri facias would have attached upon it, when gathered, yet if the claimants obtained possession on the', '')]\n",
            "[('after', 'it merely inhibits the levy, but the lien attaches, and a levy and sale may be made', '')]\n",
            "[('more than', 'taking this to be clear *744 law, and it will be seen, that the defendant in execution at the time of the levy had nothing', '')]\n",
            "[('before', 'it has been frequently mooted whether, at common law, corn, &c.,', '')]\n",
            "[('before', '**4 the statute which presents the question', '')]\n",
            "[('after', 'now, if the view taken by the majority of the court, is correct, the right secured to the plaintiff in execution, of levying on the crop', '')]\n",
            "[('before', 'tried', '')]\n",
            "[('before', 'tried', '')]\n",
            "[('before', 'tried', '')]\n",
            "[('before', 'tried', '')]\n",
            "[('before', 'tried', '')]\n",
            "[('before', 'tried', '')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6Net_6CJ1Se",
        "colab_type": "code",
        "outputId": "c913ceb3-746f-4e7a-e8dc-9cfa4afd29f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import lexnlp.extract.en.copyright\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        " for line in x:\n",
        "       cool = line.split(\" \\n \")\n",
        "       copyrights = list(lexnlp.extract.en.copyright.get_copyright(line))\n",
        "       if len(copyrights)>0:\n",
        "          print(copyrights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('©', '2019', 'Thomson Reuters. No')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Kc-qh5KXsG",
        "colab_type": "code",
        "outputId": "27455887-f953-43e9-e37c-b9783bf45707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "!pip install lexnlp\n",
        "import pandas\n",
        "import lexnlp.extract.en.courts\n",
        "from lexnlp.extract.en.dict_entities import entity_config\n",
        "\n",
        "#text = \"To be heard in either E.D. Va. or S.D.N.Y.\"\n",
        "court_df = pandas.read_csv(\"https://raw.githubusercontent.com/LexPredict/lexpredict-legal-dictionary/1.0.5/en/legal/us_courts.csv\")\n",
        "#create config objects\n",
        "court_config_data = []\n",
        "for _, row in court_df.iterrows():\n",
        "  c = entity_config(row[\"Court ID\"], row[\"Court Name\"], 0, row[\"Alias\"].split(\";\") if not pandas.isnull(row[\"Alias\"]) else [])\n",
        " #court_config_data.append(c)\n",
        "\n",
        "court_config_data = [entity_config(0, \"Supreme Court of Alabama\", 0, [\"S.C.O.A Al.\"]),\n",
        "                     entity_config(1, \"Western District of Virginia\", 0, [\"W.D. Va.\"])]\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        y = line.split(\" \\n \")\n",
        "        for entity, alias in lexnlp.extract.en.courts.get_courts(line, court_config_data):\n",
        "          print(\"entity=\", entity)\n",
        "          print(\"alias=\", alias)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lexnlp in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: typing==3.6.2 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (3.6.2)\n",
            "Requirement already satisfied: pandas==0.23.4 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.23.4)\n",
            "Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (3.4.5)\n",
            "Requirement already satisfied: pycountry==18.5.26 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (18.5.26)\n",
            "Requirement already satisfied: scipy==1.0.0 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (1.0.0)\n",
            "Requirement already satisfied: Unidecode==0.4.21 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.4.21)\n",
            "Requirement already satisfied: gensim==3.4.0 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (3.4.0)\n",
            "Requirement already satisfied: us==1.0.0 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (1.0.0)\n",
            "Requirement already satisfied: regex==2017.9.23 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (2017.9.23)\n",
            "Requirement already satisfied: reporters-db==1.0.12.1 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (1.0.12.1)\n",
            "Requirement already satisfied: datefinder-lexpredict==0.6.2 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.6.2)\n",
            "Requirement already satisfied: num2words==0.5.7 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.5.7)\n",
            "Requirement already satisfied: requests==2.22.0 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (2.22.0)\n",
            "Requirement already satisfied: dateparser==0.7.0 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.7.0)\n",
            "Requirement already satisfied: scikit-learn==0.21.3 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.21.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.4->lexnlp) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.4->lexnlp) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.4->lexnlp) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5->lexnlp) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.4.0->lexnlp) (1.11.1)\n",
            "Requirement already satisfied: jellyfish==0.5.6 in /usr/local/lib/python3.6/dist-packages (from us==1.0.0->lexnlp) (0.5.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (3.0.4)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from dateparser==0.7.0->lexnlp) (1.5.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3->lexnlp) (0.14.1)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.4.0->lexnlp) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.4.0->lexnlp) (1.12.43)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.43 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (1.15.43)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (0.15.2)\n",
            "entity= (0, 'Supreme Court of Alabama', 0, [('Supreme Court of Alabama', None, False, None, ' supreme court of alabama '), ('S.C.O.A Al.', None, False, None, ' s . c . o . a al . ')])\n",
            "alias= ('Supreme Court of Alabama', None, False, None, ' supreme court of alabama ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BXeI7ROKiXb",
        "colab_type": "code",
        "outputId": "2b1b0526-5542-4a27-a655-4c932a4cb7f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "############\n",
        "import lexnlp.extract.en.cusip\n",
        "\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        "    for line in x:\n",
        "        cool = line.split(\" \\n \")\n",
        "        cusipNum =list(lexnlp.extract.en.cusip.get_cusip(line))\n",
        "        if len(cusipNum)>0:\n",
        "           print(cusipNum)\n",
        "    print(\"None\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8EVsABNLUio",
        "colab_type": "code",
        "outputId": "0dec7c8a-03e3-4b7f-dbd8-e490b756517c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "###########\n",
        "import lexnlp.extract.en.dates\n",
        "\n",
        "\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        "    for line in x:\n",
        "        cool = line.split(\" \\n \")\n",
        "        dts =list(lexnlp.extract.en.dates.get_dates(line))\n",
        "        if len(dts)>0:\n",
        "           print(dts)\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[datetime.date(1840, 11, 1), datetime.date(1839, 10, 1), datetime.date(1840, 5, 1), datetime.date(1840, 9, 1)]\n",
            "[datetime.date(1840, 5, 1)]\n",
            "[datetime.date(1840, 5, 1)]\n",
            "[datetime.date(2020, 12, 1)]\n",
            "[datetime.date(1887, 5, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u0EMuYQLjFS",
        "colab_type": "code",
        "outputId": "25a7113f-a8d2-4337-b796-494434f4c940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import lexnlp.extract.en.definitions\n",
        "\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        "    for line in x:\n",
        "        cool = line.split(\" \\n \")\n",
        "        definition =list(lexnlp.extract.en.definitions.get_definitions(line))\n",
        "        if len(definition)>0:\n",
        "           print(definition)\n",
        "    print(\"None\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pojbWeeL03H",
        "colab_type": "code",
        "outputId": "3e152d1f-7544-4856-d851-542ed4307bde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import lexnlp.extract.en.distances\n",
        "distances_list=[]\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        "    for line in x:\n",
        "        cool = line.split(\" \\n \")\n",
        "        dist =list(lexnlp.extract.en.distances.get_distances(line))\n",
        "        if len(dist)>0:\n",
        "           distances_list.append(dist)\n",
        "if len(distances_list) == 0:\n",
        "  print(None)\n",
        "else:\n",
        "  print(distances_list)            \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk9nL2AQL-uR",
        "colab_type": "code",
        "outputId": "41cbd7bd-ca58-418c-9514-7f56a5ec5f56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import lexnlp.extract.en.durations\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        "    for line in x:\n",
        "        cool = line.split(\" \\n \")\n",
        "        durations =list(lexnlp.extract.en.durations.get_durations(line))\n",
        "        if len(durations)>0:\n",
        "           print(durations)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('second', 20.0, 0.00023148148148148146)]\n",
            "[('year', 6.0, 2190.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2C5Rd31MuoK",
        "colab_type": "code",
        "outputId": "3f1982eb-10bc-43c1-92e7-280ff003e6d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import lexnlp.extract.en.money\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        "    for line in x:\n",
        "        cool = line.split(\" \\n \")\n",
        "        money = list(lexnlp.extract.en.money.get_money(line))\n",
        "        if len(money)>0:\n",
        "           print(money)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(100.0, 'USD'), (14000, 'USD')]\n",
            "[(14000, 'USD')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcaaMQaQM5Ux",
        "colab_type": "code",
        "outputId": "5a1ca6b7-d3b6-4f70-f926-5fe258de38f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import lexnlp.extract.en.percents\n",
        "percents_list=[]\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        "    for line in x:\n",
        "        cool = line.split(\" \\n \")\n",
        "        percent = list(lexnlp.extract.en.percents.get_percents(line))\n",
        "        if len(percent)>0:\n",
        "           percents_list.append(percent)\n",
        "if len(percents_list) == 0:\n",
        "  print(None)\n",
        "else:\n",
        "  print(percents_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhVloOHYNN1V",
        "colab_type": "code",
        "outputId": "759be8e5-c1d8-4e9a-9ab9-9ce1ef720690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import lexnlp.extract.en.pii\n",
        "phone_numlist=[]\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        "    for line in x:\n",
        "        cool = line.split(\" \\n \")\n",
        "        phone = list(lexnlp.extract.en.pii.get_pii(line))\n",
        "        if len(phone)>0:\n",
        "           phone_numlist.append(phone)\n",
        "if len(phone_numlist) == 0:\n",
        "  print(None)\n",
        "else:\n",
        "  print(phone_numlist)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZTdffM8NR2l",
        "colab_type": "code",
        "outputId": "2d2f654b-d298-45ea-86e2-a1a4565c6b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import lexnlp.extract.en.ratios\n",
        "ratio_list=[]\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        "    for line in x:\n",
        "        cool = line.split(\" \\n \")\n",
        "        ratio = list(lexnlp.extract.en.ratios.get_ratios(line))\n",
        "        if len(ratio)>0:\n",
        "           ratio_list.append(ratio)\n",
        "if len(ratio_list) == 0:\n",
        "  print(None)\n",
        "else:\n",
        "  print(ratio_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmWTwe2-Nlfz",
        "colab_type": "code",
        "outputId": "b8bb9636-7efe-4f87-b143-a2e20f12b89f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import lexnlp.extract.en.regulations\n",
        "regulations_list=[]\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        "    for line in x:\n",
        "        cool = line.split(\" \\n \")\n",
        "        regulations = list(lexnlp.extract.en.regulations.get_regulations(line))\n",
        "        if len(regulations)>0:\n",
        "           regulations_list.append(regulations)\n",
        "if len(regulations_list) == 0:\n",
        "  print(None)\n",
        "else:\n",
        "  print(regulations_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nlL0NSiNvPQ",
        "colab_type": "code",
        "outputId": "c8d8ced0-6979-49f2-cc23-304dcb16cd37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import lexnlp.extract.en.trademarks\n",
        "trademarks_list=[]\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        "    for line in x:\n",
        "        cool = line.split(\" \\n \")\n",
        "        trademark = list(lexnlp.extract.en.trademarks.get_trademarks(line))\n",
        "        if len(trademark)>0:\n",
        "          trademarks_list.append(trademark)\n",
        "if len(trademarks_list) == 0:\n",
        "  print(None)\n",
        "else:\n",
        "  print(trademarks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1uhuAToOJ3a",
        "colab_type": "code",
        "outputId": "9c17c3fa-c495-4925-bb32-2c4fe7541519",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import lexnlp.extract.en.urls\n",
        "urls_list=[]\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        "    for line in x:\n",
        "        cool = line.split(\" \\n \")\n",
        "        urls = list(lexnlp.extract.en.urls.get_urls(line))\n",
        "        if len(urls)>0:\n",
        "           urls_list.append(urls)\n",
        "if len(urls_list) == 0:\n",
        "  print(None)\n",
        "else:\n",
        "  print(urls_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuY_QjnrOQ0E",
        "colab_type": "code",
        "outputId": "8fa54172-2a37-4455-a32c-3452801d24d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "!pip install pyap\n",
        "import re\n",
        "import pyap\n",
        "from pyap import utils\n",
        "from pyap.packages import six\n",
        "import pyap.source_US.data as data_us\n",
        "\n",
        "address_list=[]\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        "    for line in x:\n",
        "        cool = line.split(\" \\n \")\n",
        "        address = pyap.parse(line, country='US')\n",
        "        if address:    \n",
        "          address_list.append(address)\n",
        "if len(address_list) == 0:\n",
        "  print(\"No addresses found\")\n",
        "else:\n",
        "  print(address_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyap\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/23/aba40258c6a9fbe73a177653b900d6e6a8ab5413e57bda3c3d32f5f1cbd0/pyap-0.2.0-py2.py3-none-any.whl (76kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 51kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: pyap\n",
            "Successfully installed pyap-0.2.0\n",
            "No addresses found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MalkRScbO0sx",
        "colab_type": "code",
        "outputId": "3a5f8df3-8ce2-4468-f8f6-5b91a42d0591",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import spacy\n",
        "import re\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "persons_names =[]\n",
        "\n",
        "\n",
        "with open('/content/01-05-1  Adams v Tanner.txt', 'r') as x:\n",
        "    for line in x:\n",
        "       cool = line.split(\" \\n \")\n",
        "       cool_1 = nlp(line)\n",
        "       for ent in cool_1.ents:\n",
        "          if(ent.label_ == 'PERSON'):\n",
        "              numArray = re.findall(r'[0-9]+', ent.text) \n",
        "              if(len(numArray) == 0):\n",
        "                persons_names.append(ent.text)\n",
        "\n",
        "persons_names = list( dict.fromkeys(persons_names)) \n",
        "print((persons_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['June Term', 'Lien', 'lien', 'lien attach', 'Allen Harrison', 'R. H. SMITH', 'W. M. MURPHY', 'W. G. JONES', 'C. J.\\n', 'Whipple v. Foot', 'Stewart v. Doughty', 'Austin v. Sawyer', 'Elliott v. Mayfield', 'Dane', 'Poole', 'Mansony', 'Hurtell', 'Wood', 'Booker', 'Jones', 'M. J. SAFFOLD', 'Marshall', 'JOHN D. CUNNINGHAM', 'Jan Term', 'McKenzie', 'Hon', 'Evans', 'Lamar', 'Dewey v. Bowman', 'Jacob S. Cohen', 'Cohen', 'Jul Term', 'L. WHITLOCK', 'Edwards', 'Thompson', 'Austin', 'Sawyer', 'A. quit-claimed', 'W.', 'Perkins', 'Mayfield', 'Stewart', 'Doughty']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6YOQzx-PcpQ",
        "colab_type": "code",
        "outputId": "ef98a7d2-875e-4c44-f81b-34c1ead7a90d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        }
      },
      "source": [
        "!pip install pysbd\n",
        "!pip install lexnlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.6/dist-packages (0.2.3)\n",
            "Requirement already satisfied: lexnlp in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: us==1.0.0 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (1.0.0)\n",
            "Requirement already satisfied: regex==2017.9.23 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (2017.9.23)\n",
            "Requirement already satisfied: gensim==3.4.0 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (3.4.0)\n",
            "Requirement already satisfied: scikit-learn==0.21.3 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.21.3)\n",
            "Requirement already satisfied: scipy==1.0.0 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (1.0.0)\n",
            "Requirement already satisfied: dateparser==0.7.0 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.7.0)\n",
            "Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (3.4.5)\n",
            "Requirement already satisfied: reporters-db==1.0.12.1 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (1.0.12.1)\n",
            "Requirement already satisfied: datefinder-lexpredict==0.6.2 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.6.2)\n",
            "Requirement already satisfied: typing==3.6.2 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (3.6.2)\n",
            "Requirement already satisfied: requests==2.22.0 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (2.22.0)\n",
            "Requirement already satisfied: pycountry==18.5.26 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (18.5.26)\n",
            "Requirement already satisfied: pandas==0.23.4 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.23.4)\n",
            "Requirement already satisfied: num2words==0.5.7 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.5.7)\n",
            "Requirement already satisfied: Unidecode==0.4.21 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.4.21)\n",
            "Requirement already satisfied: jellyfish==0.5.6 in /usr/local/lib/python3.6/dist-packages (from us==1.0.0->lexnlp) (0.5.6)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim==3.4.0->lexnlp) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.4.0->lexnlp) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.4.0->lexnlp) (1.11.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3->lexnlp) (0.14.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from dateparser==0.7.0->lexnlp) (2018.9)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from dateparser==0.7.0->lexnlp) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from dateparser==0.7.0->lexnlp) (2.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (3.0.4)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.4.0->lexnlp) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.4.0->lexnlp) (1.12.43)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.43 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (1.15.43)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh4E6kmPPtIs",
        "colab_type": "code",
        "outputId": "620a9efb-f90f-459e-e380-1731c4268ad8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4ZRCWTpPy3y",
        "colab_type": "code",
        "outputId": "09550d0a-9a0e-4abd-8067-a810dbf9170f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        }
      },
      "source": [
        "!pip install -U nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2017.9.23)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.38.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434675 sha256=1cb6ba116864f6b7f90b3cb0acb8811719971b51194d034d8d97567ac2ffe4ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "\u001b[31mERROR: lexnlp 1.4.0 has requirement nltk==3.4.5, but you'll have nltk 3.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.4.5\n",
            "    Uninstalling nltk-3.4.5:\n",
            "      Successfully uninstalled nltk-3.4.5\n",
            "Successfully installed nltk-3.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1MDDj3HP6gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}