{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9",
        "colab_type": "text"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF",
        "colab_type": "text"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k",
        "colab_type": "text"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [2019 Dell labtop](https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1) on amazon.\n",
        "\n",
        "(2) Collect the top 100 User Reviews of the film [Joker](https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv) from IMDB.\n",
        "\n",
        "(3) Collect the abstracts of the top 100 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 100 tweets by using hashtag [\"#wuhancoronovirus\"](https://twitter.com/hashtag/wuhancoronovirus) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab_type": "code",
        "outputId": "26218b69-ee67-4298-f70f-2bc6f9956fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "joker = \"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"\n",
        "page = urllib.request.urlopen(joker)\n",
        "soup = BeautifulSoup(page)\n",
        "review = soup.find('div', class_='lister-item-content')\n",
        "print(review)\n",
        "all_reviews = []\n",
        "for row in review.findAll('div',class_='review-container'):\n",
        "  cells = row.findAll('span', class_='display-name-link')\n",
        "  title = row.findAll('a',class_='title')\n",
        "  if len(cells)==100:\n",
        "    comments = []\n",
        "    comments.append(cells[0].find(text=True).strip(\"\\n\"))\n",
        "    comments.append(cells[25].find(text=True).strip(\"\\n\"))\n",
        "    comments.append(cells[50].find(text=True).strip(\"\\n\"))\n",
        "    comments.append(cells[75].find(text=True).strip(\"\\n\"))\n",
        "    comments.append(cells[100].find(text=True).strip(\"\\n\"))\n",
        "    all_reviews.append(comments)\n",
        "    print(all_reviews)\n",
        "all_new_review_data = []\n",
        "heading = ['Name','Review','Title']\n",
        "for record in all_reviews:\n",
        "  record_dict = dict(int(heading,record))\n",
        "  all_new_review_data.append(record_dict)\n",
        "  print(all_new_review_data)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<div class=\"lister-item-content\">\n",
            "<div class=\"ipl-ratings-bar\">\n",
            "<span class=\"rating-other-user-rating\">\n",
            "<svg class=\"ipl-icon ipl-star-icon \" fill=\"#000000\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\" xmlns=\"http://www.w3.org/2000/svg\">\n",
            "<path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n",
            "<path d=\"M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21z\"></path>\n",
            "<path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n",
            "</svg>\n",
            "<span>10</span><span class=\"point-scale\">/10</span>\n",
            "</span>\n",
            "</div>\n",
            "<a class=\"title\" href=\"/review/rw5112402/\"> As a viewer that actually went to TIFF and witnessed this film and didn't want to believe the hype, it is an absolute MASTERPIECE and Phoenix is a certified legend.\n",
            "</a> <div class=\"display-name-date\">\n",
            "<span class=\"display-name-link\"><a href=\"/user/ur107586329/\">JF500</a></span><span class=\"review-date\">10 September 2019</span>\n",
            "</div>\n",
            "<div class=\"content\">\n",
            "<div class=\"text show-more__control\">I was a person that saw all the hype and claims of masterpiece as overreacting and overblown excitement for another Joker based film. I thought this looked solid at best and even a bit too pretentious in the trailer, but in here to say I was incredibly wrong. This is a massive achievement of cinema that's extremely rare in a day and age of cgi nonsense and reboots. While this is somewhat of a reboot of sorts, the standalone origin tale is impeccable from start to finish and echoes resemblance to the best joker origin comics from the past. Joaquin bleeds, sweats, and cries his every drop into this magnificently dedicated performance. Heath Ledger would be proud. This is undoubtedly the greatest acting performance since Heath's joker. The directing and writing is slickly brilliant and the bleak settings and tones are palpable throughout. When this film was over the place was blown away and every audience member was awestruck that they witnessed a film that could still transport them into a character's world and very existence. Believe the hype. This is going to be revered as a transcending masterpiece of cinema.</div>\n",
            "<div class=\"actions text-muted\">\n",
            "                    6,879 out of 7,916 found this helpful.\n",
            "                        <span>\n",
            "                            Was this review helpful? <a href=\"/registration/signin\"> Sign in</a> to vote.\n",
            "                        </span>\n",
            "<br/>\n",
            "<a href=\"/review/rw5112402/\">Permalink</a>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z",
        "colab_type": "text"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data_url = \"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"\n",
        "movie_reviews = pd.read_csv(data_url)\n",
        "\n",
        "#removal punctution and lowercase\n",
        "\n",
        "movie_reviews['review'] = movie_reviews['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "movie_reviews['review'].head()\n",
        "\n",
        "#removel of stop word\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "stop_word = stopwords.words('english')\n",
        "movie_reviews['stopwords'] = movie_reviews['review'].apply(lambda x: len([x for in x.split() if x in stop]))\n",
        "movie_reviews[[('review'),'stopwords']].head()\n",
        "\n",
        "#removal of numeric/number\n",
        "\n",
        "movie_reviews['numerics'] = movie_reviews['review'].apply(lambda x: len([x for in x.split() if x.isdigit()]))\n",
        "movie_reviews[['review','numerics']].head()\n",
        "\n",
        "#tokenization\n",
        "nltk.download('punkt')\n",
        "TextBlob(movie_reviews['review'][1].words)\n",
        "\n",
        "#stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "stemm = PorterStemmer()\n",
        "movie_reviews['review'][:5].apply(lambda x: \"  \".join([st.ste(word) for word in x.split()]))\n",
        "\n",
        "#lemmatization\n",
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "movie_reviews['review'] = movie_reviews['review'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "movie_reviews['review'].head()\n",
        "\n",
        "clean_review_data = []\n",
        "heading = ['Name','Review','Title']\n",
        "for record in clean_review_data:\n",
        "  record_dict = dict(int(heading,record))\n",
        "  clean_review_data.append(record_dict)\n",
        "  \n",
        "  print(clean_review_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV",
        "colab_type": "text"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#i refer this work on youtube and I didnt copy this\n",
        "# Write your code here\n",
        "#part 1\n",
        "import nltk \n",
        "from nltk.corpus import joker\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "tag_pos = joker.raw(\"review-container\")\n",
        "tag_punkt_pos = PunktSentenceTokenizer(tag_pos)\n",
        "\n",
        "text_tok = tag_punkt_pos.tokenize(sample_text)\n",
        "\n",
        "def process content():\n",
        "  try:\n",
        "      for f in tokenized:\n",
        "        words = nltk.word_tokenized(i)\n",
        "        tagging = nltk.pos_tag(words)\n",
        "\n",
        "        print(tagging)\n",
        "\n",
        "    except exception as i:\n",
        "      print(str(i))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10B43oDYOiM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#part 2\n",
        "#constituency and dependecy parsing\n",
        "import nltk\n",
        "from nltk.data import find\n",
        "from bllipparser import RerankingParser\n",
        "\n",
        "tree_parsing = find('/review/rw5092831/?ref_=tt_urv').path\n",
        "parsing = RerankingParser.from_unified_tree_parsing(tree_parsing)\n",
        "\n",
        "input_tree = prser.parser(\"I like the joker movie very much. This movie is very sad and exciting to watch.\")\n",
        "\n",
        "print(input_tree.reranker_best())\n",
        "print(input_tree.parser_best())\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18gTCgdseUKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "\n",
        "def url_to_string(url):\n",
        "  imbd = requests.get(url)\n",
        "  reviews = imbd.text\n",
        "  soup = BeautifulSoup(html, 'parser')\n",
        "  for script in soup ([\"name\", \"date\", 'user reviews']):\n",
        "    script.extract()\n",
        "  return \" \".join(re.split(r'[\\n\\t]+',soup.get_text()))\n",
        "\n",
        "the_url = \n",
        "url_to_string('https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv')\n",
        "joker_review = nlp(the_url)\n",
        "len(joker_review.ents)\n",
        "\n",
        "labels = [x.label_for x in joker_review.ents]\n",
        "Counter(labels)\n",
        "\n",
        "things = [x.text for x in joker_review.ents]\n",
        "Counter(things).most_common(1)\n",
        "\n",
        "text = [x for x in joker_revie.sents]\n",
        "print(senteces[20])\n",
        "\n",
        "dict([(str(x), x.label_) for x in nlp(str(sentences[20])).ents])\n",
        "\n",
        "print([(x, x.ent_iob_, x.ent_type_) for x in sentences[20]])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy",
        "colab_type": "text"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lNXHIp6lkmz",
        "colab_type": "text"
      },
      "source": [
        "Dependency parsing is used to define any grammatical structure sentence by listing each word as a node adn tryto lonl to its dependents.\n",
        "\n",
        "Constituency parsing tree usese contex free grammar to display syntactic structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJuzSZdtljae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7niAPhCulhob",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}